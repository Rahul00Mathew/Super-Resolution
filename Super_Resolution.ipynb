{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NgJUwspIva-W",
        "ff3fnT2Qve4E",
        "h-KNCXGMzRuQ",
        "PsseWAoN1oHZ",
        "ScSn2CbCxI6V",
        "xJ-nT4SQwAEI"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSjSsW0FuoLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f984756c-97cf-4580-d620-59a8621b3309"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import math\n",
        "\n",
        "import glob\n",
        "from pandas.core.common import flatten\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "\n",
        "import torchvision.transforms.functional as FT\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Models"
      ],
      "metadata": {
        "id": "NgJUwspIva-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Convolutional Block"
      ],
      "metadata": {
        "id": "ff3fnT2Qve4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride=1, BatchNorm=False, activation=None):\n",
        "    super(ConvolutionalBlock, self).__init__()\n",
        "\n",
        "    if activation is not None:\n",
        "      activation = activation.lower()\n",
        "      assert activation in {'prelu', 'leakyrelu', 'tanh'}\n",
        "\n",
        "    layers = []\n",
        "\n",
        "    layers.append(\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                  padding=kernel_size // 2)\n",
        "    )\n",
        "\n",
        "    if BatchNorm is True:\n",
        "      layers.append(nn.BatchNorm2d(out_channels))\n",
        "\n",
        "    if activation == 'prelu':\n",
        "      layers.append(nn.PReLU())\n",
        "    elif activation == 'leakyrelu':\n",
        "      layers.append(nn.LeakyReLU(0.2))\n",
        "    elif activation == 'tanh':\n",
        "      layers.append(nn.Tanh())\n",
        "\n",
        "    self.conv = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.conv(x)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "uBVocoz7viaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sub-Pixel Convolutional Block"
      ],
      "metadata": {
        "id": "h-KNCXGMzRuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SubPixelConvolutionalBlock(nn.Module):\n",
        "  def __init__(self, n_channels=64, kernel_size=3, scaling_factor=2):\n",
        "    super(SubPixelConvolutionalBlock, self).__init__()\n",
        "\n",
        "    self.conv = nn.Conv2d(in_channels=n_channels, out_channels=n_channels*(scaling_factor**2),\n",
        "                          kernel_size=kernel_size, padding=kernel_size // 2)\n",
        "    self.pixel_shuffle = nn.PixelShuffle(upscale_factor=scaling_factor)\n",
        "    self.prelu = nn.PReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.conv(x)\n",
        "    output = self.pixel_shuffle(output)\n",
        "    output = self.prelu(output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "ey1lCl-SzXHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Residual Block"
      ],
      "metadata": {
        "id": "PsseWAoN1oHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, n_channels=64, kernel_size=3):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "\n",
        "    self.conv_block1 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=3,\n",
        "                                          BatchNorm=True, activation='prelu')\n",
        "\n",
        "    self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=3,\n",
        "                                          BatchNorm=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "\n",
        "    output = self.conv_block1(x)\n",
        "    output = self.conv_block2(x)\n",
        "    output = output + identity\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "mT2sWXEAeijw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SRResNet"
      ],
      "metadata": {
        "id": "ScSn2CbCxI6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SRResNet(nn.Module):\n",
        "  def __init__(self, large_kernel_size=3, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n",
        "    super(SRResNet, self).__init__()\n",
        "\n",
        "    scaling_factor = int(scaling_factor)\n",
        "    assert scaling_factor in {2, 4, 8}, 'The scaling factor must be 2, 4, 8.'\n",
        "\n",
        "    self.conv1 = ConvolutionalBlock(in_channels=3, out_channels=n_channels, kernel_size=large_kernel_size,\n",
        "                                    BatchNorm=False, activation='prelu')\n",
        "\n",
        "    self.res_blocks = nn.Sequential(\n",
        "        *[ResidualBlock(n_channels=n_channels, kernel_size=small_kernel_size) for i in range(n_blocks)]\n",
        "    )\n",
        "\n",
        "    self.conv2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=large_kernel_size,\n",
        "                                    BatchNorm=True, activation=None)\n",
        "\n",
        "    n_subpixel_blocks = int(math.log2(scaling_factor))\n",
        "    self.subpixel_blocks = nn.Sequential(\n",
        "        *[SubPixelConvolutionalBlock(n_channels=n_channels, kernel_size=small_kernel_size, scaling_factor=2)\n",
        "        for i in range(n_subpixel_blocks)]\n",
        "    )\n",
        "\n",
        "    self.conv3 = ConvolutionalBlock(in_channels=n_channels, out_channels=3, kernel_size=large_kernel_size,\n",
        "                                    BatchNorm=False, activation='Tanh')\n",
        "\n",
        "  def forward(self, lr_img):\n",
        "    output = self.conv1(lr_img)\n",
        "\n",
        "    residual = output\n",
        "\n",
        "    output = self.res_blocks(output)\n",
        "    output = self.conv2(output)\n",
        "\n",
        "    output = output + residual\n",
        "\n",
        "    output = self.subpixel_blocks(output)\n",
        "    sr_img = self.conv3(output)\n",
        "\n",
        "    return sr_img"
      ],
      "metadata": {
        "id": "B0KguI34xO01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generator"
      ],
      "metadata": {
        "id": "r3ho3_dGZuUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, large_kernel_size=3, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    self.generator = SRResNet(large_kernel_size=large_kernel_size, small_kernel_size=small_kernel_size,\n",
        "                              n_channels=n_channels, n_blocks=n_blocks, scaling_factor=scaling_factor)\n",
        "\n",
        "  def init_with_srresnet(self, srresnet_checkpoint):\n",
        "    srresnet = torch.load(srresnet_checkpoint)\n",
        "    self.generator.load_state_dict(srresnet.state_dict())\n",
        "\n",
        "    print('\\nLoaded weights from pre-trained SRResNet.\\n')\n",
        "\n",
        "  def forward(self, lr_img):\n",
        "    sr_img = self.generator(lr_img)\n",
        "\n",
        "    return sr_img"
      ],
      "metadata": {
        "id": "TmP5DBzEZ-xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Discrimiator"
      ],
      "metadata": {
        "id": "kShrFOSUbqUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels=3, n_channels=64, kernel_size=3, n_blocks=7, fc_size=1024):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.conv1 = ConvolutionalBlock(in_channels=in_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
        "                                    BatchNorm=False, activation='leakyrelu')\n",
        "\n",
        "    in_channels = n_channels\n",
        "    conv_blocks = []\n",
        "    for i in range(1, n_blocks+1):\n",
        "\n",
        "      if i % 2 == 0:\n",
        "        out_channels = in_channels * 2\n",
        "        stride = 1\n",
        "      else:\n",
        "        out_channels = in_channels\n",
        "        stride = 2\n",
        "\n",
        "      conv_blocks.append(ConvolutionalBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                           BatchNorm=True, activation='LeakyReLU'))\n",
        "      in_channels = out_channels\n",
        "    self.conv_blocks = nn.Sequential(*conv_blocks)\n",
        "\n",
        "    self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "    self.fc1 = nn.Linear(out_channels * 6 * 6, fc_size)\n",
        "\n",
        "    self.leakyrelu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.fc2 = nn.Linear(fc_size, 1)\n",
        "\n",
        "  def forward(self, hr_img):\n",
        "    N = hr_img.shape[0]\n",
        "\n",
        "    output = self.conv1(hr_img)\n",
        "    output = self.conv_blocks(output)\n",
        "    output = self.adaptive_avg_pool(output)\n",
        "    output = self.fc1(output.view(N, -1))\n",
        "    output = self.leakyrelu(output)\n",
        "    logit = self.fc2(output)\n",
        "\n",
        "    return logit"
      ],
      "metadata": {
        "id": "3IEimfUQb8FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Truncated VGG19"
      ],
      "metadata": {
        "id": "zprTxnZQhX9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TruncatedVGG19(nn.Module):\n",
        "  def __init__(self, i, j):\n",
        "    # In this class we are looking for truncated vgg19 with (i-1) maxpool layers\n",
        "    # and j convolutional layers after the (i-1)th maxpool layer\n",
        "\n",
        "    super(TruncatedVGG19, self).__init__()\n",
        "\n",
        "    vgg19 = torchvision.models.vgg19(pretrained=True)\n",
        "\n",
        "    truncate_at = 0\n",
        "    conv_count = 0\n",
        "    pool_count = 0\n",
        "\n",
        "    for layer in vgg19.features.children():\n",
        "      truncate_at += 1\n",
        "\n",
        "      # Count the number of maxpool layers and the convolutional layers after each maxpool\n",
        "      if isinstance(layer, nn.Conv2d):\n",
        "        conv_count += 1\n",
        "      if isinstance(layer, nn.MaxPool2d):\n",
        "        pool_count += 1\n",
        "        conv_count = 0\n",
        "\n",
        "      # Break after reaching jth convolutional layer after (i-1)th maxpool\n",
        "      if pool_count == i - 1 and conv_count == j:\n",
        "        break\n",
        "\n",
        "    assert pool_count == i - 1 and conv_count == j, f'One or both of i = {i} and j = {j} are not valid choices for VGG19!'\n",
        "\n",
        "    # Include ReLU after the Convolutional layer\n",
        "    self.new_vgg19 = nn.Sequential(*list(vgg19.features.children())[:truncate_at + 1])\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.new_vgg19(x)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "GavvQchwkGID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utils"
      ],
      "metadata": {
        "id": "xJ-nT4SQwAEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_folders = r'/content/drive/MyDrive/ImageNet100/train.X'\n",
        "\n",
        "test_folders = [r'/content/drive/MyDrive/SR test dataset/BSD100',\n",
        "                r'/content/drive/MyDrive/SR test dataset/Set14',\n",
        "                r'/content/drive/MyDrive/SR test dataset/Set5']\n",
        "\n",
        "output_folder = r'/content/drive/MyDrive/Projects/Super Resolution'\n",
        "\n",
        "min_size = 100"
      ],
      "metadata": {
        "id": "iom_NnCwvB_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_lists(train_folders, test_folders, min_size, output_folder):\n",
        "\n",
        "  print(\"\\nCreating data lists... this may take some time.\\n\")\n",
        "\n",
        "  train_images = []\n",
        "  for data_path in glob.glob(train_folders + '/*'):\n",
        "    train_images.append(glob.glob(data_path + '/*'))\n",
        "\n",
        "  train_images = list(flatten(train_images))\n",
        "\n",
        "#  for img_path in train_images:\n",
        "#    img = Image.open(img_path, mode='r')\n",
        "#    if img.width >= min_size and img.height >= min_size:\n",
        "#      train_images.append(img_path)\n",
        "\n",
        "  random.seed(42)\n",
        "  random.shuffle(train_images)\n",
        "\n",
        "  print(f\"There are {len(train_images)} images in the training data.\\n\")\n",
        "  with open(os.path.join(output_folder, 'train_images.json'), 'w') as j:\n",
        "    json.dump(train_images, j)\n",
        "\n",
        "\n",
        "  for folder in test_folders:\n",
        "    test_images = []\n",
        "    test_name = folder.split('/')[-1]\n",
        "    for path in glob.glob(folder + '/*'):\n",
        "      img = Image.open(path, mode='r')\n",
        "      if img.width >= min_size and img.height >= min_size:\n",
        "        test_images.append(path)\n",
        "    print(f'There are {len(test_images)} images in {test_name} dataset')\n",
        "    with open(os.path.join(output_folder, test_name + '_test_images.json'), 'w') as j:\n",
        "      json.dump(test_images, j)\n",
        "\n",
        "  print(f\"\\nJSONS containing lists of Train and Test images have been saved to {output_folder}.\\n\")"
      ],
      "metadata": {
        "id": "l8x7JWE6oaeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create_data_lists(train_folders, test_folders, min_size, output_folder)"
      ],
      "metadata": {
        "id": "up7GKj8NwCfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rgb_weights = torch.FloatTensor([65.481, 128.553, 24.966]).to(device)\n",
        "imagenet_mean = torch.FloatTensor([0.485, 0.456, 0.406]).unsqueeze(1).unsqueeze(2)\n",
        "imagenet_std = torch.FloatTensor([0.229, 0.224, 0.225]).unsqueeze(1).unsqueeze(2)\n",
        "imagenet_mean_cuda = torch.FloatTensor([0.485, 0.456, 0.406]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
        "imagenet_std_cuda = torch.FloatTensor([0.229, 0.224, 0.225]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)"
      ],
      "metadata": {
        "id": "nA3HDWJLNU2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_image(img, source, target):\n",
        "\n",
        "  assert source in {'pil', '[0, 1]', '[-1, 1]'}, f'Cannot convert from source format: {source}'\n",
        "  assert target in {'pil', '[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet_norm',\n",
        "                    'y_channels'}, f'Cannot convert to target format: {target}'\n",
        "\n",
        "  # Convert from source to [0, 1]\n",
        "  if source == 'pil':\n",
        "    img = FT.to_tensor(img)\n",
        "  elif source == '[0, 1]':\n",
        "    pass\n",
        "  elif source == '[-1, 1]':\n",
        "    img = (img + 1.0) /2\n",
        "\n",
        "  # Convert from source to target\n",
        "  if target == 'pil':\n",
        "    img = FT.to_pil_image(img)\n",
        "  elif target == '[0, 255]':\n",
        "    img = 255.0 * img\n",
        "  elif target == '[0, 1]':\n",
        "    pass\n",
        "  elif target == '[-1, 1]':\n",
        "    img = 2.0 * img - 1.0\n",
        "\n",
        "  elif target == 'imagenet_norm':\n",
        "    if img.ndimension() == 3:\n",
        "      img = (img - imagenet_mean) / imagenet_std\n",
        "    elif img.ndimension() == 4:\n",
        "      img = (img - imagenet_mean_cuda) / imagenet_std_cuda\n",
        "\n",
        "  # y_channels is for converting the image from RGB to YCbCr format for finding Peak Signal-to-Noise Ratio (PSNR)\n",
        "  # and Structural Similarity Index Measure (SSIM). This is not used for training.\n",
        "  elif target == 'y_channels':\n",
        "    img = torch.matmul(255.0 * img.permute(0, 2, 3, 1)[:, 4:-4, 4:-4, :], rgb_weights) / 255.0 + 16\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "DM1k50ZO0a-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTransform():\n",
        "  def __init__(self, split, crop_size, scaling_factor, LR_img_type, HR_img_type):\n",
        "\n",
        "    self.split = split.lower()\n",
        "    self.crop_size = crop_size\n",
        "    self.scaling_factor = scaling_factor\n",
        "    self.LR_img_type = LR_img_type\n",
        "    self.HR_img_type = HR_img_type\n",
        "\n",
        "    assert split in {'train', 'test'}\n",
        "\n",
        "  def __call__(self, img):\n",
        "\n",
        "    if self.split == 'train':\n",
        "      # Take a random fixed-size crop of the image, which will serve as the high-resolution (HR) image\n",
        "      left = random.randint(1, img.width - self.crop_size)\n",
        "      top = random.randint(1, img.height - self.crop_size)\n",
        "      right = left + self.crop_size\n",
        "      bottom = top + self.crop_size\n",
        "    else:\n",
        "      # Take the largest possible center-crop such that its dimensions are divisible by the scaling factor\n",
        "      x = img.width % self.scaling_factor\n",
        "      y = img.height % self.scaling_factor\n",
        "      left = x // 2\n",
        "      top = y // 2\n",
        "      right = left + img.width - x\n",
        "      bottom = top + img.height - y\n",
        "\n",
        "    HR_img = img.crop((left, top, right, bottom))\n",
        "\n",
        "    # Dowsample the High Resolution crop using Bicubic downsampling to obtain Low Resolution version of the image\n",
        "    LR_img = HR_img.resize((int(HR_img.width / self.scaling_factor), int(HR_img.height / self.scaling_factor)),\n",
        "                           Image.BICUBIC)\n",
        "\n",
        "    assert HR_img.width == LR_img.width * self.scaling_factor and HR_img.height == LR_img.height * self.scaling_factor\n",
        "\n",
        "\n",
        "    LR_img = convert_image(LR_img, source='pil', target=self.LR_img_type)\n",
        "    HR_img = convert_image(HR_img, source='pil', target=self.HR_img_type)\n",
        "\n",
        "    return LR_img, HR_img"
      ],
      "metadata": {
        "id": "gJzkC1kPiU_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Metric:\n",
        "  def __init__(self, len_loader):\n",
        "    self.values = []\n",
        "    self.epochs = []\n",
        "    self.len_loader = len_loader\n",
        "\n",
        "  def reset(self):\n",
        "    self.val = 0\n",
        "\n",
        "  def update(self, value, epoch):\n",
        "    self.values.append(value)\n",
        "    self.epochs.append(epoch)\n",
        "\n",
        "  def add(self, val, n, epoch):\n",
        "    self.val += val\n",
        "    if n == self.len_loader:\n",
        "      self.avg = self.val / n\n",
        "      self.update(value=self.avg, epoch=epoch)"
      ],
      "metadata": {
        "id": "aqAhaxP1R88M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset"
      ],
      "metadata": {
        "id": "cOEQJaq8mC57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SRDataset(Dataset):\n",
        "  def __init__(self, data_folder, split, crop_size, scaling_factor, LR_img_type, HR_img_type, test_data_name=None):\n",
        "\n",
        "    self.data_folder = data_folder\n",
        "    self.split = split.lower()\n",
        "    self.crop_size = crop_size\n",
        "    self.scaling_factor = scaling_factor\n",
        "    self.LR_img_type = LR_img_type\n",
        "    self.HR_img_type = HR_img_type\n",
        "    self.test_data_name = test_data_name\n",
        "\n",
        "    assert self.split in {'train', 'test'}\n",
        "    if self.split == 'test' and self.test_data_name is None:\n",
        "      raise ValueError('Provide the name of the test dataset!')\n",
        "    assert LR_img_type in {'[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet_norm'}\n",
        "    assert HR_img_type in {'[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet_norm'}\n",
        "\n",
        "    if self.split == 'train':\n",
        "      assert self.crop_size % self.scaling_factor == 0, 'Crop dimensions are not perfectly divisible by the scaling factor!'\n",
        "\n",
        "    if self.split == 'train':\n",
        "      with open(os.path.join(data_folder, 'train_images.json'), 'r') as j:\n",
        "        self.images = json.load(j)\n",
        "    else:\n",
        "      with open(os.path.join(data_folder, self.test_data_name + '_test_images.json'), 'r') as j:\n",
        "        self.images = json.load(j)\n",
        "\n",
        "    self.transform = ImageTransform(split=self.split,\n",
        "                                    crop_size=self.crop_size,\n",
        "                                    scaling_factor=self.scaling_factor,\n",
        "                                    LR_img_type=self.LR_img_type,\n",
        "                                    HR_img_type=self.HR_img_type)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img = Image.open(self.images[idx], mode='r')\n",
        "    img = img.convert('RGB')\n",
        "    if img.width <= 96 or img.height <= 96:\n",
        "      print(self.images[idx], img.width, img.height)\n",
        "    LR_img, HR_img = self.transform(img)\n",
        "\n",
        "    return LR_img, HR_img\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)"
      ],
      "metadata": {
        "id": "1v2yIujEMsAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "ibxhJnYlMhCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SRResNet"
      ],
      "metadata": {
        "id": "dMqbGhtYMi7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "t-yVsa7iM6gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data parameters"
      ],
      "metadata": {
        "id": "Ezdp88Y7N1oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = output_folder\n",
        "crop_size = 96\n",
        "scaling_factor = 4"
      ],
      "metadata": {
        "id": "sLsBdQajNJIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Model Parameters"
      ],
      "metadata": {
        "id": "-TYvYFcFNX9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "large_kernel_size = 9\n",
        "small_kernel_size = 3\n",
        "n_channels = 64\n",
        "n_blocks = 16"
      ],
      "metadata": {
        "id": "ntNzMGEaN_G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Learning parameters"
      ],
      "metadata": {
        "id": "oBD_JY4mOMQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = None\n",
        "batch_size = 16\n",
        "start_epoch = 0\n",
        "iteration = 1e6\n",
        "num_workers = 4\n",
        "print_every = 500\n",
        "lr = 1e-4"
      ],
      "metadata": {
        "id": "b_lO3idfOTvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "MVcxQjxsOp0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SRDataset(data_folder,\n",
        "                          split='train',\n",
        "                          crop_size=crop_size,\n",
        "                          scaling_factor=scaling_factor,\n",
        "                          LR_img_type='imagenet_norm',\n",
        "                          HR_img_type='[-1, 1]')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
        "                          pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrJ1Ioi7eSpY",
        "outputId": "ad02f49a-8218-4cca-86fb-41c368e8b51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v86gauzqjOge",
        "outputId": "152ead97-bdf9-469a-f3e4-f1a99406987a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8125"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training SRResNet"
      ],
      "metadata": {
        "id": "knk5mQa9sQYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = int(1e6 // len(train_loader))\n",
        "epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnt-36UAjXf8",
        "outputId": "be470ebe-4ed7-4ce2-b594-7ed00be529aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if checkpoint is None:\n",
        "  model = SRResNet(large_kernel_size=large_kernel_size, small_kernel_size=small_kernel_size,\n",
        "                   n_channels=n_channels, n_blocks=n_blocks, scaling_factor=scaling_factor)\n",
        "  # Initialize the optimizer\n",
        "  optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                               lr=lr)\n",
        "\n",
        "else:\n",
        "  checkpoint = torch.load(checkpoint)\n",
        "  start_epoch = checkpoint['epoch'] + 1\n",
        "  model = checkpoint['model']\n",
        "  optimizer = checkpoint['optimizer']\n",
        "\n",
        "criterion = nn.MSELoss().to(device)"
      ],
      "metadata": {
        "id": "JSv4_LPHm4mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_SRResNet(train_loader, model, criterion, optimizer, epoch, print_every=100):\n",
        "\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  train_loss = 0\n",
        "\n",
        "  for batch, (LR_imgs, HR_imgs) in enumerate(train_loader):\n",
        "\n",
        "    LR_imgs = LR_imgs.to(device)\n",
        "    HR_imgs = HR_imgs.to(device)\n",
        "\n",
        "    # 1. Forward pass\n",
        "    SR_imgs = model(LR_imgs)\n",
        "\n",
        "    # 2. Calculate loss\n",
        "    loss = criterion(SR_imgs, HR_imgs)\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss\n",
        "\n",
        "    if (batch + 1) % print_every == 0:\n",
        "      print(f'Epoch: {epoch}| Batch: {batch+1}| loss: {loss:.4f}')\n",
        "\n",
        "  train_loss /= len(train_loader)\n",
        "\n",
        "  del LR_imgs, HR_imgs, SR_imgs\n",
        "\n",
        "  return train_loss"
      ],
      "metadata": {
        "id": "93IP4V5yOs1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 0\n",
        "\n",
        "PATH_checkpoint_srresnet = r'/content/drive/MyDrive/Projects/Super Resolution'\n",
        "name = '/checkpoint_SRResNet.pth'\n",
        "PATH_checkpoint_srresnet += name\n",
        "\n",
        "torch.save({'epoch': epoch,\n",
        "            'model': model,\n",
        "            'checkpoint': checkpoint},\n",
        "           PATH_checkpoint_srresnet)\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "epoch_list = []\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "  train_loss = train_SRResNet(train_loader=train_loader,\n",
        "                     model=model,\n",
        "                     criterion=criterion,\n",
        "                     optimizer=optimizer,\n",
        "                     epoch=epoch,\n",
        "                     print_every=print_every)\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  epoch_list.append(epoch)\n",
        "\n",
        "  torch.save({'epoch': epoch,\n",
        "              'model': model,\n",
        "              'optimizer': optimizer},\n",
        "             PATH_checkpoint_srresnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "l2rhYoyvmSqZ",
        "outputId": "43f5ce95-f6c5-4679-f1d9-4f2402cd1c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntrain_losses = []\\nepoch_list = []\\n\\nfor epoch in range(start_epoch, epochs):\\n  train_loss = train_SRResNet(train_loader=train_loader,\\n                     model=model,\\n                     criterion=criterion,\\n                     optimizer=optimizer,\\n                     epoch=epoch,\\n                     print_every=print_every)\\n\\n  train_losses.append(train_loss)\\n  epoch_list.append(epoch)\\n\\n  torch.save({'epoch': epoch,\\n              'model': model,\\n              'optimizer': optimizer},\\n             'checkpoint_srresnet.pth.tar')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_srresnet = r'/content/drive/MyDrive/Projects/Super Resolution'\n",
        "name = '/SRResNet.pth'\n",
        "PATH_srresnet += name\n",
        "\n",
        "torch.save(model, PATH_srresnet)"
      ],
      "metadata": {
        "id": "tDGne7gW54O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SRGAN"
      ],
      "metadata": {
        "id": "1un7YWX9uldy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Generator Parameters"
      ],
      "metadata": {
        "id": "I_iz7R6gur6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "large_kernel_size_g = 9\n",
        "small_kernel_size_g = 3\n",
        "n_channels_g = 64\n",
        "n_blocks_g = 16\n",
        "srresnet_checkpoint = PATH_srresnet"
      ],
      "metadata": {
        "id": "Tl2xh3V4urio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Discriminator Parameters"
      ],
      "metadata": {
        "id": "rSKoeg6R2NM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_size_d = 3\n",
        "n_channels_d = 64\n",
        "n_blocks_d = 7\n",
        "fc_size_d = 1024"
      ],
      "metadata": {
        "id": "lV1i5gQd2SAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Learning Parameters"
      ],
      "metadata": {
        "id": "69OPJ5aD2ktq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_g = None\n",
        "batch_size = 16\n",
        "start_epoch_g = 0\n",
        "iteration_g = 2e5\n",
        "num_workers = 4\n",
        "vgg19_i = 5\n",
        "vgg19_j = 4\n",
        "beta = 1e-3\n",
        "print_every = 500\n",
        "lr_g = 1e-4"
      ],
      "metadata": {
        "id": "pOmq8PpK2qg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "aMpewCHE3TNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if checkpoint_g is None:\n",
        "  # Generator\n",
        "  generator = Generator(large_kernel_size=large_kernel_size_g,\n",
        "                        small_kernel_size=small_kernel_size_g,\n",
        "                        n_channels=n_channels_g,\n",
        "                        n_blocks=n_blocks_g)\n",
        "\n",
        "  # Initialize with generator pre-trained SRResNet\n",
        "  generator.init_with_srresnet(srresnet_checkpoint)\n",
        "\n",
        "  # Initialize generator's optimizer\n",
        "  optimizer_g = torch.optim.Adam(generator.parameters(), lr=lr_g)\n",
        "\n",
        "  # Discriminator\n",
        "  discriminator = Discriminator(n_channels=n_channels_d,\n",
        "                                kernel_size=kernel_size_d,\n",
        "                                n_blocks=n_blocks_d,\n",
        "                                fc_size=fc_size_d)\n",
        "\n",
        "  # Initialize discriminator's optimizer\n",
        "  optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=lr_g)\n",
        "\n",
        "else:\n",
        "  checkpoint_g = torch.load(checkpoint_g)\n",
        "  start_epoch_g = checkpoint_g['epoch'] + 1\n",
        "  generator = checkpoint_g['generator']\n",
        "  discriminator = checkpoint_g['discriminator']\n",
        "  optimizer_g = checkpoint_g['optimizer_g']\n",
        "  optimizer_d = checkpoint_g['optimizer_d']\n",
        "  print(f'\\nLoaded checkpoint from epoch {start_epoch_g}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9v9kkz_3Wwe",
        "outputId": "0cb55fe2-2203-42cc-a3d2-95c5b4b5b2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded weights from pre-trained SRResNet.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncated VGG19 to calculate loss in vgg space\n",
        "truncated_vgg19 = TruncatedVGG19(i=vgg19_i, j=vgg19_j)\n",
        "truncated_vgg19.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn-CwB0J9vDW",
        "outputId": "8cfa1c58-ec22-461a-e1ae-6429de64388c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:07<00:00, 76.5MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TruncatedVGG19(\n",
              "  (new_vgg19): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace=True)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions\n",
        "content_loss_criterion = nn.MSELoss()\n",
        "adversarial_loss_criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "_DwaMRNhIfLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_g = SRDataset(data_folder,\n",
        "                            split='train',\n",
        "                            crop_size=crop_size,\n",
        "                            scaling_factor=scaling_factor,\n",
        "                            LR_img_type='imagenet_norm',\n",
        "                            HR_img_type='imagenet_norm')\n",
        "\n",
        "train_dataloader_g = DataLoader(train_dataset_g, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
        "                                pin_memory=True)"
      ],
      "metadata": {
        "id": "RngvJMSAi66T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training SRGAN"
      ],
      "metadata": {
        "id": "AfLcXTsjCLFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_g = int(iteration_g // len(train_dataloader_g)) + 1\n",
        "epochs_g"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW3dapHToFwn",
        "outputId": "6e77b83b-eead-440c-c914-bb60ea138cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_SRGAN(train_loader_g, generator, discriminator, truncated_vgg19, content_loss_criterion,\n",
        "                adversarial_loss_criterion, optimizer_g, optimizer_d, beta, epoch, print_every=100):\n",
        "\n",
        "  # Move models to device\n",
        "  generator.to(device)\n",
        "  discriminator.to(device)\n",
        "\n",
        "  # Put models in train mode\n",
        "  generator.train()\n",
        "  discriminator.train()\n",
        "\n",
        "  for batch, (LR_imgs, HR_imgs) in enumerate(train_loader_g):\n",
        "\n",
        "    # Move to default device\n",
        "    LR_imgs, HR_imgs = LR_imgs.to(device), HR_imgs.to(device)\n",
        "\n",
        "    # GENERATOR Update\n",
        "    SR_imgs = generator(LR_imgs)\n",
        "    # SR_imgs are output by SRResNet trained to output in [-1, 1] convert them to\n",
        "    # imagenet_norm for content loss in VGG space\n",
        "    SR_imgs = convert_image(SR_imgs, source='[-1, 1]', target='imagenet_norm')\n",
        "\n",
        "    SR_imgs_vgg_space = truncated_vgg19(SR_imgs)\n",
        "    HR_imgs_vgg_space = truncated_vgg19(HR_imgs).detach() # by detaching, we turn off the gradients since they are constants\n",
        "\n",
        "    # Pass SR_imgs to discriminator\n",
        "    SR_imgs_discriminated = discriminator(SR_imgs)\n",
        "\n",
        "    content_loss = content_loss_criterion(SR_imgs_vgg_space, HR_imgs_vgg_space)\n",
        "    adversarial_loss_g = adversarial_loss_criterion(SR_imgs_discriminated, torch.ones_like(SR_imgs_discriminated))\n",
        "    perceptual_loss = content_loss + beta * adversarial_loss_g\n",
        "\n",
        "    # Optimizer zero grad\n",
        "    optimizer_g.zero_grad()\n",
        "\n",
        "    # Loss backward\n",
        "    perceptual_loss.backward()\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer_g.step()\n",
        "\n",
        "\n",
        "    # DISCRIMINATOR Update\n",
        "    HR_imgs_discriminated = discriminator(HR_imgs)\n",
        "    SR_imgs_discriminated = discriminator(SR_imgs.detach()) # By detaching SR_imgs before passing to the discriminator\n",
        "    # we ensure that the backpropagation is stopped at the discriminator and the gradient does not flow to the Generator\n",
        "\n",
        "    adversarial_loss_d = adversarial_loss_criterion(SR_imgs_discriminated, torch.zeros_like(SR_imgs_discriminated)) + \\\n",
        "    adversarial_loss_criterion(HR_imgs_discriminated, torch.ones_like(HR_imgs_discriminated))\n",
        "    # In case of 1st loss, when passed SR_imgs should be driven down to zeros\n",
        "    # and for 2nd loss, HR_imgs should be driven up to ones\n",
        "\n",
        "    # Optimizer zero grad\n",
        "    optimizer_d.zero_grad()\n",
        "\n",
        "    # Loss backward\n",
        "    adversarial_loss_d.backward()\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer_d.step()\n",
        "\n",
        "    if (batch + 1) % print_every == 0:\n",
        "      print(f'Epoch: {epoch} | Batch: {batch+1} | Content Loss = {content_loss} | Adversarial Loss Generator = {adversarial_loss_g}\\\n",
        "      Adversaria Loss Discriminator = {adversarial_loss_d}')\n",
        "\n",
        "  del LR_imgs, HR_imgs, SR_imgs, HR_imgs_vgg_space, SR_imgs_vgg_space, HR_discriminated, SR_discriminated"
      ],
      "metadata": {
        "id": "2aafGo9VoaHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(start_epoch_g, epochs_g):\n",
        "\n",
        "  train_SRGAN(train_loader_g=train_dataloader_g,\n",
        "              generator=generator,\n",
        "              discriminator=discriminator,\n",
        "              truncated_vgg19=truncated_vgg19,\n",
        "              content_loss_criterion=content_loss_criterion,\n",
        "              adversarial_loss_criterion=adversarial_loss_criterion,\n",
        "              optimizer_g=optimizer_g,\n",
        "              optimizer_d=optimizer_d,\n",
        "              beta=beta,\n",
        "              epoch=epoch)\n",
        "\n",
        "  '''\n",
        "  torch.save({'epoch': epoch,\n",
        "              'generator': generator,\n",
        "              'discriminator': discriminator,\n",
        "              'optimizer_g': optimizer_g,\n",
        "              'optimizer_d': optimizer_d},\n",
        "             PATH_checkpoint_srgan)'''"
      ],
      "metadata": {
        "id": "mfeOtHnIzaha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_srgan = r'/content/drive/MyDrive/Projects/Super Resolution'\n",
        "name = '/SRGAN.pth'\n",
        "PATH_srgan += name\n",
        "\n",
        "torch.save(generator, PATH_srgan)"
      ],
      "metadata": {
        "id": "su2OBoZDUWXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfOh8qsZU8E5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}